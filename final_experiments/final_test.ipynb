{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This notebook takes all wordwise decisions from the individual models and layers them to test selected combinations of models in the TEST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "words_train = pd.read_csv('../data/train_wordwise_clean.csv')\n",
    "words_dev = pd.read_csv('../data/dev_wordwise_clean.csv')\n",
    "words_test = pd.read_csv('../data/test_wordwise_clean.csv')\n",
    "\n",
    "## testing data in fragments\n",
    "frag_train = pd.read_csv('../data/all_train_aligned.csv')\n",
    "frag_dev = pd.read_csv('../data/all_dev_aligned.csv')\n",
    "frag_test = pd.read_csv('../data/all_test_aligned.csv')\n",
    "\n",
    "\n",
    "frag_train = frag_train[frag_train.apply(lambda x: type(x['0']) == str, axis = 1)]\n",
    "frag_dev = frag_dev[frag_dev.apply(lambda x: type(x['0']) == str, axis = 1)]\n",
    "frag_test = frag_test[frag_test.apply(lambda x: type(x['0']) == str, axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../models/BERT/pickled_results/test_wordwise_decisions.pkl', 'rb') as f:\n",
    "  bert_test_wordwise_decisions = pickle.load(f)\n",
    "\n",
    "with open('../models/LEX/lexicon_decisions_test.pkl', 'rb') as f:\n",
    "  lexicon_test = pickle.load(f)\n",
    "\n",
    "with open('../models/MLE/mle_85_decisions_test.pkl', 'rb') as f:\n",
    "  mle_85_test = pickle.load(f)\n",
    "\n",
    "with open('../models/MLE/mle_0_decisions_test.pkl', 'rb') as f:\n",
    "  mle_0_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess BERT outputs\n",
    "Our BERT model is limited to 20 tokens, which is enough for a large majority of the fragments but is short for some exceptionally long ones. We pad with zeroes to equal the lengths of the decision arrays from other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check preformance when padded to 30? (only lose around 150 tokens)\n",
    "bert_test_padded_decisions = []\n",
    "\n",
    "for x, y in zip(mle_0_test, bert_test_wordwise_decisions):\n",
    "  y = y + [0 for x in range(len(x['levels']) - len(y))]\n",
    "  bert_test_padded_decisions.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create default level vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_0 = []\n",
    "levels_3 = []\n",
    "levels_4 = []\n",
    "levels_5 = []\n",
    "\n",
    "for x in mle_0_test:\n",
    "  levels_0.append([0 for x in range(len(x['levels']))])\n",
    "  levels_3.append([3 for x in range(len(x['levels']))])\n",
    "  levels_4.append([4 for x in range(len(x['levels']))])\n",
    "  levels_5.append([5 for x in range(len(x['levels']))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ground truth\n",
    "The ground truth is included with the output of every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = [e['gts'] for e in mle_0_test]\n",
    "\n",
    "wordwise_gt = np.concatenate(test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounds levels 1-2 (returned sometimes by LEX) to 3\n",
    "\n",
    "def level_keep0(l):\n",
    "  if l > 0:\n",
    "    if l < 3:\n",
    "      return 3\n",
    "    else:\n",
    "      return l\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = {\n",
    "    'levels_0': np.concatenate([e['levels'] for e in levels_0]),\n",
    "    'levels_3': np.concatenate([e['levels'] for e in levels_3]),\n",
    "    'levels_4': np.concatenate([e['levels'] for e in levels_4]),\n",
    "    'levels_5': np.concatenate([e['levels'] for e in levels_5]),\n",
    "    'levels_mle': np.concatenate([e['levels'] for e in mle_0_test]),\n",
    "    'levels_mle_85': np.concatenate([e['levels'] for e in mle_85_test]),\n",
    "    'levels_lexicon': [level_keep0(l) for l in np.concatenate([e['levels'] for e in lexicon_test])],\n",
    "    'levels_bert': [x+3 for x in np.concatenate(bert_test_padded_decisions)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_experiment_pipeline(decision_1, decision_2 = decisions['levels_0'], decision_final = decisions['levels_0']):\n",
    "  final_result = []\n",
    "  ### DECISION 2\n",
    "  for i, x in enumerate(decision_1):\n",
    "    if x == 0:\n",
    "      final_result.append(decision_2[i])\n",
    "    else:\n",
    "      final_result.append(x)\n",
    "\n",
    "  ### Final decision\n",
    "  for i, x in enumerate(final_result):\n",
    "    if x == 0:\n",
    "      final_result[i] = decision_final[i]\n",
    "\n",
    "  return final_result\n",
    "\n",
    "exps = [\n",
    "    comb_experiment_pipeline(decisions['levels_mle_85'], decision_2 = decisions['levels_lexicon'], decision_final = decisions['levels_bert']),\n",
    "    comb_experiment_pipeline(decisions['levels_mle'], decision_2 = decisions['levels_0'], decision_final = decisions['levels_bert']),\n",
    "    comb_experiment_pipeline(decisions['levels_lexicon'], decision_2 = decisions['levels_0'], decision_final = decisions['levels_bert']),\n",
    "    comb_experiment_pipeline(decisions['levels_bert'], decision_2 = decisions['levels_0'], decision_final = decisions['levels_3']),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordwise_gt = test_gt.astype(int)\n",
    "def results_to_csv(result_arr):\n",
    "  all_rows = []\n",
    "  for resu in result_arr:\n",
    "    inv_report = classification_report(final_gt, resu, output_dict = True)\n",
    "\n",
    "    arr_inv = np.concatenate([[inv_report[x]['f1-score'],\n",
    "            inv_report[x]['precision'],\n",
    "            inv_report[x]['recall'],] for x in ['3', '4', '5']])\n",
    "    arr_inv = np.append(arr_inv, inv_report['accuracy'])\n",
    "    arr_inv = np.append(arr_inv, inv_report['macro avg']['f1-score'])\n",
    "\n",
    "    all_rows.append(arr_inv)\n",
    "\n",
    "  return all_rows\n",
    "\n",
    "all_rows = results_to_csv(exps)\n",
    "\n",
    "df_results_words = pd.DataFrame(all_rows, columns = ['f1_3','3_prec','3_recall','f1_4','4_prec','4_recall','f1_5','5_prec','5_recall','accuracy','f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation into Fragment Level Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_0 = [x['levels'] for x in levels_0]\n",
    "frag_3 = [x['levels'] for x in levels_3]\n",
    "frag_4 = [x['levels'] for x in levels_4]\n",
    "frag_5 = [x['levels'] for x in levels_5]\n",
    "frag_mle = [x['levels'] for x in mle_0_test]\n",
    "frag_mle_85 = [x['levels'] for x in mle_85_test]\n",
    "frag_lexicon = [[l if l > 3 or l == 0 else 3 for l in x['levels']] for x in lexicon_test]\n",
    "frag_bert = [[w+3 for w in frag] for frag in bert_test_padded_decisions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frag_exps_pipeline(decision_1, decision_2 = frag_0, decision_final = frag_0, frags = frag_dev['0']):\n",
    "  all_results = []\n",
    "  for d1, d2, d3, f in zip(decision_1, decision_2, decision_final, frags):\n",
    "    toks = [t.split('#')[0] for t in f.split(' ')]\n",
    "    gts = [int(t.split('#')[1]) for t in f.split(' ')]\n",
    "    gold_level = max(gts)\n",
    " \n",
    "    decision = [dec if dec != 0 else alt for dec, alt in zip(d1, d2)]\n",
    "\n",
    "    decision = [dec if dec != 0 else alt for dec, alt in zip(decision, d3)]\n",
    "\n",
    "    pred = max(decision)\n",
    "\n",
    "    all_results.append([\n",
    "        gold_level,\n",
    "        pred\n",
    "    ])\n",
    "  return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = [\n",
    "\n",
    "    frag_exps_pipeline(frag_mle_85, decision_2 = frag_lexicon, decision_final = frag_bert),\n",
    "    frag_exps_pipeline(frag_mle, decision_2 = frag_0, decision_final = frag_bert),\n",
    "    frag_exps_pipeline(frag_lexicon, decision_2 = frag_0, decision_final = frag_bert),\n",
    "    frag_exps_pipeline(frag_bert, decision_2 = frag_0, decision_final = frag_3),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frag_results_to_csv(result_arr):\n",
    "  all_rows = []\n",
    "  for resu in result_arr:\n",
    "    rr = pd.DataFrame(resu, columns = ['gt', 'pred'])\n",
    "\n",
    "    inv_report = classification_report(rr['gt'], rr['pred'], output_dict = True)\n",
    "\n",
    "    arr_inv = np.concatenate([[inv_report[x]['f1-score'],\n",
    "            inv_report[x]['precision'],\n",
    "            inv_report[x]['recall'],] for x in ['3', '4', '5']])\n",
    "    arr_inv = np.append(arr_inv, inv_report['accuracy'])\n",
    "    arr_inv = np.append(arr_inv, inv_report['macro avg']['f1-score'])\n",
    "\n",
    "    all_rows.append(arr_inv)\n",
    "\n",
    "  return all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_df = pd.DataFrame(frag_results_to_csv(exps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
