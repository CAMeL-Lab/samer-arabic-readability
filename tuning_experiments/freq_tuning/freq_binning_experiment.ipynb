{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37305,"status":"ok","timestamp":1712081667319,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"uKydpWKHJtdn","outputId":"25d5a632-577a-46df-ae60-cf05bcde239f"},"outputs":[],"source":["from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report"]},{"cell_type":"markdown","metadata":{"id":"94bkwj8RKphD"},"source":["### Approach:\n","\n","Divide the 11M word corpus into equally sized 10000 word bins. Count, within these bins, the number of 3, 4, and 5s. Assign to each bin the majority category.\n","\n","Build a dict words `{word: bin}`\n","\n","Build a dict bins `{bin: [l3, l4, l5]}`\n","\n","Use the majority of each bin to label the words above."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18826,"status":"ok","timestamp":1712081686131,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"nswiMbhMNKE8"},"outputs":[],"source":["cleaned_freqs = pd.read_csv('../../data/freq/all_camelbert_freqs.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1712081686134,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"JiFOflDAWdBQ"},"outputs":[],"source":["import pickle"]},{"cell_type":"markdown","metadata":{"id":"r0dQ5QH-TvNG"},"source":["- Get all our known, labelled words and place them into the bins"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1009,"status":"ok","timestamp":1712082453444,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"z4eu2ucCTxWb"},"outputs":[],"source":["with open('../../data/levels_db/mle_max_aligned_model.pkl', 'rb') as f:\n","  known_levels = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"VzXbNkt8_EiW"},"source":["- Do this for bins of equal size in terms of cumulative frequency"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1712082131631,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"f3HdLL4SBFb5"},"outputs":[],"source":["total_words = sum(cleaned_freqs['1'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1712084215469,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"uglGxZ8P3QT_"},"outputs":[],"source":["def level_full_pipeline_eq_sized(n_bins, oov_level = 5):\n","\n","  bin_size = total_words/n_bins\n","  words = {}\n","  bins = {i: {3: 0, 4: 0, 5: 0} for i in range(0, n_bins)}\n","\n","  bin_words = []\n","\n","  current_sum = 0\n","  current_bin = 0\n","  words_in_bin = 0\n","  for word, freq in zip(cleaned_freqs['0'], cleaned_freqs['1']):\n","    words[word] = current_bin\n","    current_sum += freq\n","    words_in_bin += 1\n","    if current_sum > bin_size:\n","      current_sum = current_sum % bin_size\n","      current_bin += 1\n","      bin_words.append(words_in_bin)\n","      words_in_bin = 0\n","\n","  for word, level in known_levels.items():\n","    try:\n","      bin = words[word]\n","      bins[bin][level] += 1\n","    except:\n","      pass\n","\n","\n","  no_hits_bins = 0\n","\n","  bins_levelled = {}\n","  for bin in bins.keys():\n","    if list(bins[bin].values()).count(0) == 3:\n","      bins_levelled[bin] = oov_level\n","      no_hits_bins += 1\n","    else:\n","      bins_levelled[bin] = max(bins[bin].items(), key = lambda x: x[1])[0]\n","\n","  levelled_words = {}\n","  for word in words:\n","    levelled_words[word] = bins_levelled[words[word]]\n","\n","  return levelled_words, bin_words\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126797,"status":"ok","timestamp":1712084350124,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"oTyo87myFXfv","outputId":"a0086770-47ff-4339-820a-d8fc9dd67b8e"},"outputs":[],"source":["bincounts = [10000, 5000, 2000, 1000, 500, 200, 100]\n","level_sets = {bc: level_full_pipeline_eq_sized(bc) for bc in bincounts}"]},{"cell_type":"markdown","metadata":{},"source":["### Benchmarking"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24454,"status":"ok","timestamp":1712084374559,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"t0uwanKZHalk"},"outputs":[],"source":["with open('../../data/freq/all_levels_freq_binning_cumulative.pkl', 'wb') as f:\n","  pickle.dump(level_sets, f)\n","\n","def get_rl(token, model, oov_level = 3):\n","  try:\n","      return model[token]\n","  except:\n","      return oov_level\n","\n","## testing data in fragments\n","frag_train = pd.read_csv('../../data/all_train_aligned.csv')\n","frag_dev = pd.read_csv('../../data/all_dev_aligned.csv')\n","frag_test = pd.read_csv('../../data/all_test_aligned.csv')\n","\n","\n","frag_train = frag_train[frag_train.apply(lambda x: type(x['0']) == str, axis = 1)]\n","frag_dev = frag_dev[frag_dev.apply(lambda x: type(x['0']) == str, axis = 1)]\n","frag_test = frag_test[frag_test.apply(lambda x: type(x['0']) == str, axis = 1)]\n","\n","def levels_pipeline(fragment, model):\n","  tokens = [t.split('#')[0] for t in fragment.split(' ')]\n","  gt = [t.split('#')[1] for t in fragment.split(' ')]\n","\n","  # decision round 1:\n","  levels = [get_rl(token, model) for token in tokens]\n","\n","  return {\n","      'levels': levels,\n","      'gts': gt,\n","  }\n","\n","keys = list(level_sets.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIjNBviDIzHv"},"outputs":[],"source":["results = [\n","    [levels_pipeline(f, level_sets[w][0]) for f in frag_dev['0']]\n","    for w in keys\n","]\n","\n","res = [\n","    np.concatenate([e['levels'] for e in r])\n","    for r in results\n","]\n","\n","final_gt = np.concatenate([e['gts'] for e in results[0]])\n","\n","final_gt = final_gt.astype(int)\n","\n","def results_to_csv(result_arr):\n","  all_rows = []\n","  for resu, key in zip(result_arr, keys):\n","    inv_report = classification_report(final_gt, resu, output_dict = True)\n","\n","    arr_inv = np.concatenate([[inv_report[x]['f1-score'],\n","            inv_report[x]['precision'],\n","            inv_report[x]['recall'],] for x in ['3', '4', '5']])\n","    arr_inv = np.append(arr_inv, inv_report['accuracy'])\n","    arr_inv = np.append(arr_inv, inv_report['macro avg']['f1-score'])\n","    arr_inv = np.append(arr_inv, key)\n","\n","\n","    all_rows.append(arr_inv)\n","\n","  return all_rows\n","\n","pd.DataFrame(results_to_csv(res), columns= ['f1_3','3_prec','3_recall','f1_4','4_prec','4_recall','f1_5','5_prec','5_recall','accuracy','f1_macro', 'binsize'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMCq5sBjnu6iya7Vde58v52","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
