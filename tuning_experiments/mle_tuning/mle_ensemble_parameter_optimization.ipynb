{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1715461546362,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"YZw6_Ry3Ztep"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5661,"status":"ok","timestamp":1715461557637,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"soMHvsil-0U6","outputId":"9fa61164-4f69-4275-99cd-ef970c48098c"},"outputs":[],"source":["import pandas as pd\n","\n","from tqdm import tqdm\n","import re\n","from sklearn.metrics import classification_report\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","\n","import math\n","\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import os\n","\n","\n","from camel_tools.tokenizers.word import simple_word_tokenize\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10156,"status":"ok","timestamp":1715461567776,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"piZdFNQEWq4g","outputId":"cdb17b7d-870c-4b4b-84da-05aa3214d4a9"},"outputs":[],"source":["!pip install Levenshtein\n","import Levenshtein\n","from camel_tools.morphology.database import MorphologyDB\n","from camel_tools.morphology.analyzer import Analyzer\n","from camel_tools.tokenizers.word import simple_word_tokenize\n","from camel_tools.disambig.mle import MLEDisambiguator\n","from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n","from pathlib import Path\n","S31_DB_PATH = Path('../data/disambig_db/calima-msa-s31.db')\n","S31_DB = MorphologyDB(S31_DB_PATH, 'a')\n","S31_AN = Analyzer(S31_DB, 'NOAN_ALL', cache_size=100000)\n","bert_disambig = BERTUnfactoredDisambiguator.pretrained('msa', top=1000, pretrained_cache = False)\n","bert_disambig._analyzer = S31_AN"]},{"cell_type":"markdown","metadata":{"id":"qIM1y7Ba-5dP"},"source":["## Objective\n","All Wordwise Techniques and Vocabularies. Code so that we can automatically generate results.\n","\n","As well, produce summary statistics on the vocabularies: MLE, FREQ, SAMERLEX, etc.\n","\n","### Expected Training and Testing Data:\n","Punctuation tokenized, all tokens that are either all punctuation or digits removed."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715461567777,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"M6s6fGMx-_Zf"},"outputs":[],"source":["## train data\n","import numpy as np\n","words_train = pd.read_csv('../data/train_wordwise_clean.csv')\n","words_dev = pd.read_csv('../data/dev_wordwise_clean.csv')\n","words_test = pd.read_csv('../data/test_wordwise_clean.csv')\n","\n","## testing data in fragments\n","frag_train = pd.read_csv('../data/all_train_aligned.csv')\n","frag_dev = pd.read_csv('../data/all_dev_aligned.csv')\n","frag_test = pd.read_csv('../data/all_test_aligned.csv')\n","\n","\n","frag_train = frag_train[frag_train.apply(lambda x: type(x['0']) == str, axis = 1)]\n","frag_dev = frag_dev[frag_dev.apply(lambda x: type(x['0']) == str, axis = 1)]\n","frag_test = frag_test[frag_test.apply(lambda x: type(x['0']) == str, axis = 1)]"]},{"cell_type":"markdown","metadata":{"id":"Wgs-kSLxfYJO"},"source":["### Acquire BERT Decisions from BERT Notebook\n","Decisions made on levelling the dev set."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1715461567777,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"W3ULChtufb5D"},"outputs":[],"source":["import pickle\n","\n","### save wordwise decisions\n","with open('../BERT/pickled_results/res_wordwise_decisions.pkl', 'rb') as f:\n","  res_wordwise_decisions = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"1l_674OWkN86"},"source":["### Acquire FREQ vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43816,"status":"ok","timestamp":1715461611580,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"-835gfzyeoAS"},"outputs":[],"source":["import pickle\n","with open('../data/freq/freq_token_db.pkl', 'rb') as f:\n","  freq_backoff = pickle.load(f)\n","with open('../data/freq/freq_token_db.pkl/all_levels_freq_binning_cumulative.pkl', 'rb') as f:\n","  words_and_levels_freq_binning_cum = pickle.load(f)\n","\n","alt_backoff = words_and_levels_freq_binning_cum[10000][0]"]},{"cell_type":"markdown","metadata":{"id":"BAHPIqBkgzuh"},"source":["### Acquire MLE vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715461611581,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"9ObdWL77grNQ"},"outputs":[],"source":["def get_mle_counts_aligned(words, levels):\n","  dict_levels = {}\n","  for word, level in zip(words, levels):\n","      try:\n","          #assume every entry of dict_levels : {3: int, 4: int, 5: int}\n","          dict_levels[word][level] += 1\n","      except:\n","          dict_levels[word] = {3: 0, 4: 0, 5: 0}\n","          dict_levels[word][level] += 1\n","  return dict_levels\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715461611581,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"eO8UH9udShRE"},"outputs":[],"source":["# ugly, lazy global\n","stats = []\n","\n","def max_frequency_strategy(dict_levels, confidence = 0, min_occurrences = 1):\n","  dict_levels_max = {}\n","  no = 0\n","  for token in dict_levels.keys():\n","    cd = max(dict_levels[token].values())/sum(dict_levels[token].values())\n","    if cd >= confidence and sum(dict_levels[token].values()) >= min_occurrences:\n","      dict_levels_max[token] = max(dict_levels[token].items(), key = lambda x: x[1])[0]\n","    else:\n","      no += 1\n","  return dict_levels_max, no\n","\n","def mle_training_pipeline_aligned(data, strategy, confidence = 0, min_occurrences = 1):\n","  counts = get_mle_counts_aligned(data['Word'], data['Label'])\n","  dl, rejects = strategy(counts, confidence = confidence, min_occurrences = min_occurrences)\n","  stats.append([confidence, min_occurrences, rejects])\n","  return dl\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715461611582,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"3STeUJWzjVWd"},"outputs":[],"source":["### look at min number of things - cross the confidence intervals and the threshold appearance minima (1s and 2s, 3s) (cross this)\n","### look at para level disambig - add to method as tuning for the lexicon.\n","### send note to\n","\n","### do as tuning\n","### Error Analysis\n","### select a sample of a hundred cases (fragment) - word levels might be lil bit more (report on percentages, and which errors were consequential - inconsequential)\n","### Automatic classification: look at who caused the error\n"]},{"cell_type":"markdown","metadata":{"id":"y0lsgNihdrLH"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7767,"status":"ok","timestamp":1715461619334,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"Hzn5d2Fcbk9c"},"outputs":[],"source":["\n","mle_thresholds = [mle_training_pipeline_aligned(words_train, max_frequency_strategy, confidence = x/100, min_occurrences = y) for y in range(1,8) for x in range(30, 105, 5)]\n","mle_functions = [\n","    lambda token, analyses: mle.get(token, 0) for mle in mle_thresholds\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1715461619335,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"Ox4JADY1quRM","outputId":"0fde0db2-2d04-4c55-b35d-55b84f0da869"},"outputs":[],"source":["len(mle_thresholds[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715461619335,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"VfmjWrY0sJRa"},"outputs":[],"source":["stats_df = pd.DataFrame(stats, columns =['confidence', 'threshold', 'tok_rejected'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715461619336,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"ox5B2dHlkSxk"},"outputs":[],"source":["\n","# configure lexicon\n","import pickle\n","with open('../data/lemma_db/quick_lemma_lookup.pkl', 'rb') as f:\n","    lemma_db = pickle.load(f)\n","\n","\n","def sort_score(list_of_analyses):\n","  list_of_analyses.sort(key = lambda x: x.score, reverse = True)\n","  highest_score = list_of_analyses[0].score\n","  analyses_with_equal_score = [x for x in list_of_analyses\n","                                if x.score == highest_score]\n","  return analyses_with_equal_score\n","\n","\n","def sort_lexlogprob(list_of_analyses):\n","  list_of_analyses.sort(key = lambda x: x.analysis['lex_logprob'], reverse=True)\n","  highest_prob = list_of_analyses[0].analysis['lex_logprob']\n","  analyses_with_equal_prob = [x for x in list_of_analyses\n","                                if x.analysis['lex_logprob'] == highest_prob]\n","  return analyses_with_equal_prob\n","\n","def default_level_oov(word, level, analysis):\n","  return level\n","\n","def sort_level(list_of_analyses):\n","  list_of_analyses.sort(key = lambda x: get_rl_single(x.analysis, 9999))\n","  lowest_rl = get_rl_single(list_of_analyses[0].analysis, 9999)\n","  analyses_with_equal_level = [x for x in list_of_analyses\n","                                    if get_rl_single(x.analysis, 9999) == lowest_rl]\n","  return analyses_with_equal_level\n","\n","def score_then_level_then_llp(list_of_analyses):\n","  list_of_analyses = sort_score(list_of_analyses)\n","  if len(list_of_analyses) == 1:\n","    return list_of_analyses[0].analysis\n","\n","  list_of_analyses = sort_level(list_of_analyses)\n","  if len(list_of_analyses) == 1:\n","    return list_of_analyses[0].analysis\n","\n","  list_of_analyses = sort_lexlogprob(list_of_analyses)\n","  return list_of_analyses[0].analysis\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715461619337,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"rWsN3r5mjygd"},"outputs":[],"source":["def get_rl_0(token, analyses, oov_level = 0):\n","  return oov_level\n","\n","def get_rl_3(token, analyses, oov_level = 3):\n","  return oov_level\n","\n","def get_rl_4(token, analyses, oov_level = 4):\n","  return oov_level\n","\n","def get_rl_5(token, analyses, oov_level = 5):\n","  return oov_level\n","\n","\n","def get_rl_alt_freq(token, analyses, oov_level = 0):\n","    try:\n","        return alt_backoff[token]\n","    except:\n","        return oov_level\n","\n","def get_rl_freq(token, analyses, oov_level = 0):\n","    try:\n","        return freq_backoff[token]\n","    except:\n","        return oov_level\n","\n","def get_rl_single(analysis, oov_level = 0):\n","  lex = analysis['lex']\n","  pos = analysis['pos']\n","  if pos == 'noun_prop':\n","    return 3\n","  result = lemma_db.get(lex)\n","  if result:\n","    if len(result) == 1:\n","      rl = result[0]['level']\n","    else:\n","      most_similar_element = None\n","      max_similarity = -1\n","\n","      for element in result:\n","          similarity = Levenshtein.ratio(pos, element['pos'])\n","          if similarity > max_similarity:\n","              max_similarity = similarity\n","              most_similar_element = element\n","\n","      rl = most_similar_element['level']\n","    return rl\n","  else:\n","    return oov_level\n","\n","\n","def get_rl_lexicon(token, analyses, oov_level = 0):\n","  analysis = score_then_level_then_llp(analyses)\n","\n","  return get_rl_single(analysis)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1715461619337,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"qjNuQR6CnZ09"},"outputs":[],"source":["def levels_pipeline(fragment, decision_1, decision_2, requires_disambig = False):\n","  tokens = [t.split('#')[0] for t in fragment.split(' ')]\n","  gt = [t.split('#')[1] for t in fragment.split(' ')]\n","\n","  if requires_disambig:\n","    analyses = [token.analyses for token in bert_disambig.disambiguate(tokens)]\n","  else:\n","    analyses = gt\n","\n","  # decision round 1:\n","  levels = [decision_1(token, analysis) for token, analysis in zip(tokens, analyses)]\n","\n","  # decision round 2:\n","  levels = [l if l != 0 else decision_2(t, a) for l, t, a in zip(levels, tokens, analyses)]\n","\n","  return {\n","      'levels': levels,\n","      'gts': gt,\n","  }\n","\n","def mle_levels_pipeline(fragment, model):\n","  tokens = [t.split('#')[0] for t in fragment.split(' ')]\n","  gt = [t.split('#')[1] for t in fragment.split(' ')]\n","\n","  levels = [model.get(token, 0) for token in tokens]\n","\n","  return {\n","      'levels': levels\n","  }\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":161405,"status":"ok","timestamp":1715461780727,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"Zb5qwDKeYvBZ"},"outputs":[],"source":["### New Approach: calculate decisions first, then hierarchy. Saves time!\n","pure_0 = [levels_pipeline(f, get_rl_0, get_rl_0) for f in frag_dev['0']]\n","\n","\n","pure_3 = [levels_pipeline(f, get_rl_3, get_rl_3) for f in frag_dev['0']]\n","pure_4 = [levels_pipeline(f, get_rl_4, get_rl_4) for f in frag_dev['0']]\n","pure_5 = [levels_pipeline(f, get_rl_5, get_rl_5) for f in frag_dev['0']]\n","\n","pure_freq = [levels_pipeline(f, get_rl_freq, get_rl_0) for f in frag_dev['0']]\n","pure_alt_freq = [levels_pipeline(f, get_rl_alt_freq, get_rl_0) for f in frag_dev['0']]\n","\n","\n","pure_lexicon = [levels_pipeline(f, get_rl_lexicon, get_rl_0, requires_disambig = True) for f in frag_dev['0']]\n","\n","\n","def level_keep0(l):\n","  if l > 0:\n","    if l < 3:\n","      return 3\n","    else:\n","      return l\n","  else:\n","    return 0\n","\n","## Lexicon might level 1 or 2.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2531,"status":"ok","timestamp":1715461783205,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"wIbJH-K9iypw"},"outputs":[],"source":["pure_mles = [\n","    [\n","        mle_levels_pipeline(f, model) for f in frag_dev['0']\n","    ]\n","    for model in mle_thresholds\n","]"]},{"cell_type":"markdown","metadata":{"id":"OUDSrYkdVvGY"},"source":["#### We need to preprocess BERT outputs, as they are padded to 20.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1715461783205,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"y3adoi0LTGND"},"outputs":[],"source":["#### Check preformance when padded to 30? (only lose around 150 tokens)\n","bert_padded_decisions = []\n","\n","for x, y in zip(pure_alt_freq, res_wordwise_decisions[0]):\n","  y = y + [0 for x in range(len(x['levels']) - len(y))]\n","  bert_padded_decisions.append(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1715461783206,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"0rFVuF9tU4uw","outputId":"b5f39a0c-d744-4c95-ee5d-a51837c99f8f"},"outputs":[],"source":["np.concatenate(bert_padded_decisions).shape"]},{"cell_type":"markdown","metadata":{"id":"8Ex0Db46Vwg8"},"source":["#### Decisions setup"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1236,"status":"ok","timestamp":1715461784435,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"u3uPeXlVRpqk"},"outputs":[],"source":["decisions = {\n","    'pure_0': np.concatenate([e['levels'] for e in pure_0]),\n","    'pure_3': np.concatenate([e['levels'] for e in pure_3]),\n","    'pure_4': np.concatenate([e['levels'] for e in pure_4]),\n","    'pure_5': np.concatenate([e['levels'] for e in pure_5]),\n","    'pure_freq': np.concatenate([e['levels'] for e in pure_freq]),\n","    'pure_alt_freq': np.concatenate([e['levels'] for e in pure_alt_freq]),\n","    'pure_lexicon': [level_keep0(l) for l in np.concatenate([e['levels'] for e in pure_lexicon])],\n","    'pure_bert': [x+3 for x in np.concatenate(bert_padded_decisions)]\n","}\n","\n","mle_decisions = [\n","    np.concatenate([e['levels'] for e in d]) for d in pure_mles\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1715461784436,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"TDXvUnyXPkWa"},"outputs":[],"source":["final_gt = np.concatenate([e['gts'] for e in pure_freq])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1715461784436,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"3jw0P2CtjNq8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1715461784437,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"JozbcJRHPi2g"},"outputs":[],"source":["def comb_experiment_pipeline(decision_1, decision_2 = decisions['pure_0'], decision_final = decisions['pure_0']):\n","  final_result = []\n","  ### DECISION 2\n","  for i, x in enumerate(decision_1):\n","    if x == 0:\n","      final_result.append(decision_2[i])\n","    else:\n","      final_result.append(x)\n","\n","  ### Final decision\n","  for i, x in enumerate(final_result):\n","    if x == 0:\n","      final_result[i] = decision_final[i]\n","\n","  return final_result\n","\n","exps = [\n","    comb_experiment_pipeline(mle_dec, decision_2 = decisions['pure_lexicon'], decision_final = decisions['pure_bert']) for mle_dec in mle_decisions\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1715461784437,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"yjxoPwh7At-q"},"outputs":[],"source":["final_gt = final_gt.astype(int)\n","def results_to_csv(result_arr):\n","  all_rows = []\n","  for resu in result_arr:\n","    inv_report = classification_report(final_gt, resu, output_dict = True)\n","\n","    arr_inv = np.concatenate([[inv_report[x]['f1-score'],\n","            inv_report[x]['precision'],\n","            inv_report[x]['recall'],] for x in ['3', '4', '5']])\n","    arr_inv = np.append(arr_inv, inv_report['accuracy'])\n","    arr_inv = np.append(arr_inv, inv_report['macro avg']['f1-score'])\n","\n","    all_rows.append(arr_inv)\n","\n","  return all_rows\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2150,"status":"ok","timestamp":1715461786579,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"O8HzkzpEKnev"},"outputs":[],"source":["all_rows = results_to_csv(exps)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1715461786580,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"t8jw3EAmgA20"},"outputs":[],"source":["df_results_old = pd.DataFrame(all_rows, columns = ['f1_3','3_prec','3_recall','f1_4','4_prec','4_recall','f1_5','5_prec','5_recall','accuracy','f1_macro'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvVgVMVNrKp-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAIezlJ0tt0L"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"7eYiXRslRkH1"},"source":["### Aggregation Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzXO0ZTCmkH9"},"outputs":[],"source":["pure_0 = [levels_pipeline(f, get_rl_0, get_rl_0) for f in frag_dev['0']]\n","\n","\n","pure_3 = [levels_pipeline(f, get_rl_3, get_rl_3) for f in frag_dev['0']]\n","\n","pure_freq = [levels_pipeline(f, get_rl_freq, get_rl_0) for f in frag_dev['0']]\n","pure_alt_freq = [levels_pipeline(f, get_rl_alt_freq, get_rl_0) for f in frag_dev['0']]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyMEhnbLt3Bc"},"outputs":[],"source":["pure_frag_mles = [[e['levels'] for e in d] for d in pure_mles]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1715458130017,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"3iC2kEr2RXeh","outputId":"2f54520b-f40e-4223-a076-121d16fad38b"},"outputs":[],"source":["len(pure_frag_mles[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngqQ9fdMnyP_"},"outputs":[],"source":["pure_frag_0 = [x['levels'] for x in pure_0]\n","pure_frag_3 = [x['levels'] for x in pure_3]\n","\n","\n","pure_frag_lexicon = [[l if l > 3 or l == 0 else 3 for l in x['levels']] for x in pure_lexicon]\n","pure_frag_bert = [[w+3 for w in frag] for frag in bert_padded_decisions]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rAHlKW3mZ9e"},"outputs":[],"source":["\n","def frag_exps_pipeline(decision_1, decision_2 = pure_frag_0, decision_final = pure_frag_0, frags = frag_dev['0']):\n","  all_results = []\n","  for d1, d2, d3, f in zip(decision_1, decision_2, decision_final, frags):\n","    toks = [t.split('#')[0] for t in f.split(' ')]\n","    gts = [int(t.split('#')[1]) for t in f.split(' ')]\n","    gold_level = max(gts)\n","\n","    words_5 = [t for t, l in zip(toks, gts) if l == 5]\n","    words_4 = [t for t, l in zip(toks, gts) if l == 4]\n","    words_3 = [t for t, l in zip(toks, gts) if l == 3]\n","\n","    ### predictive process\n","    ## d2\n","\n","    round = ['mle' if l != 0 else 0 for l in d1]\n","\n","    decision = [dec if dec != 0 else alt for dec, alt in zip(d1, d2)]\n","\n","    round = ['lex' if r == 0 and d != 0 else r for r, d in zip(round, decision)]\n","\n","    decision = [dec if dec != 0 else alt for dec, alt in zip(decision, d3)]\n","\n","    round = ['bert' if r == 0 and d != 0 else r for r, d in zip(round, decision)]\n","\n","    pred = max(decision)\n","\n","\n","    misidentified_5 = [\"{}/{}/{}\".format(t, d, r) for t, g, d, r in zip(toks, gts, decision, round) if (d != g and g == 5)]\n","    misidentified_4 = [\"{}/{}/{}\".format(t, d, r) for t, g, d, r in zip(toks, gts, decision, round) if (d != g and g == 4)]\n","    misidentified_3 = [\"{}/{}/{}\".format(t, d, r) for t, g, d, r in zip(toks, gts, decision, round) if (d != g and g == 3)]\n","\n","    all_errors = [' '.join([\"{}->{}->{}\".format(g, r, d) for t, g, d, r in zip(toks, gts, decision, round) if (d != g and g == x)]) for x in [3,4,5]]\n","\n","    all_results.append([\n","        gold_level,\n","        pred\n","    ])\n","  return all_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGIFnu3vm4rn"},"outputs":[],"source":["exps_frags = [\n","    frag_exps_pipeline(mle_dec, decision_2 = pure_frag_lexicon, decision_final = pure_frag_bert) for mle_dec in pure_frag_mles\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLUcu1q3oa98"},"outputs":[],"source":["def frag_results_to_csv(result_arr):\n","  all_rows = []\n","  for resu in result_arr:\n","    rr = pd.DataFrame(resu, columns = ['gt', 'pred'])\n","\n","    inv_report = classification_report(rr['gt'], rr['pred'], output_dict = True)\n","\n","    arr_inv = np.concatenate([[inv_report[x]['f1-score'],\n","            inv_report[x]['precision'],\n","            inv_report[x]['recall'],] for x in ['3', '4', '5']])\n","    arr_inv = np.append(arr_inv, inv_report['accuracy'])\n","    arr_inv = np.append(arr_inv, inv_report['macro avg']['f1-score'])\n","\n","    all_rows.append(arr_inv)\n","\n","  return all_rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6748fsopv1g"},"outputs":[],"source":["frag_df = pd.DataFrame(frag_results_to_csv(exps_frags))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1715461872492,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"9yumVC9tDpYx","outputId":"590255c2-60f8-4bc6-d0e0-513bbd11373f"},"outputs":[],"source":["frag_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EuXW761rfobk"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOTAHwbi6+vHkNjbn17h1Cl","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
