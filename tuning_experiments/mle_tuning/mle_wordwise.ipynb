{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41433,"status":"ok","timestamp":1711526481175,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"kiPnyBTN3CaI","outputId":"0c482149-f988-4d11-ca8a-cc6de4361b1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["## Objective - MLE Wordwise Training and Parameter Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338745,"status":"ok","timestamp":1711528620274,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"0wB1D-kxCOR3","outputId":"2c299e4f-6812-4d2e-d1c3-de6b7de9de25"},"outputs":[],"source":["from camel_tools.tokenizers.word import simple_word_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fh1j-3Xn-jMC"},"outputs":[],"source":["## imports for training data\n","\n","base_path_aligned = '../../data/readability_data'\n","dev_aligned = pd.read_csv(base_path_aligned + '/dev_pnx.tsv', sep = '\\t')\n","test_aligned = pd.read_csv(base_path_aligned + '/test_pnx.tsv', sep = '\\t')\n","train_aligned = pd.read_csv(base_path_aligned + '/train_pnx.tsv', sep = '\\t')\n","\n","base_path = '../../data/splits/levelled_fragments/'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpkSLwFj7Kxd"},"outputs":[],"source":["frag_train = pd.read_csv('../data/all_train_aligned.csv')\n","frag_dev = pd.read_csv('../data/all_dev_aligned.csv')\n","frag_test = pd.read_csv('../data/all_test_aligned.csv')\n","\n","\n","frag_train = frag_train[frag_train.apply(lambda x: type(x['0']) == str, axis = 1)]\n","frag_dev = frag_dev[frag_dev.apply(lambda x: type(x['0']) == str, axis = 1)]\n","frag_test = frag_test[frag_test.apply(lambda x: type(x['0']) == str, axis = 1)]"]},{"cell_type":"markdown","metadata":{"id":"nrHShkFX8qMb"},"source":["### Strategy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wug_H-nW7Ohp"},"outputs":[],"source":["def get_mle_counts_aligned(words, levels):\n","  dict_levels = {}\n","  for word, level in zip(words, levels):\n","      try:\n","          #assume every entry of dict_levels : {3: int, 4: int, 5: int}\n","          dict_levels[word][level] += 1\n","      except:\n","          dict_levels[word] = {3: 0, 4: 0, 5: 0}\n","          dict_levels[word][level] += 1\n","  return dict_levels\n","\n","def max_frequency_strategy(dict_levels):\n","  dict_levels_max = {}\n","  for token in dict_levels.keys():\n","    dict_levels_max[token] = max(dict_levels[token].items(), key = lambda x: x[1])[0]\n","  return dict_levels_max\n","\n","def weighted_average_strategy(dict_levels):\n","  dict_levels_avg = {}\n","  for token in dict_levels.keys():\n","    dict_levels_avg[token] = np.average(list(dict_levels[token].keys()), weights = list(dict_levels[token].values()))\n","  return dict_levels_avg"]},{"cell_type":"markdown","metadata":{"id":"AISoueuH82KK"},"source":["### Setups"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p6DzvziJ87of"},"outputs":[],"source":["\n","\n","def mle_training_pipeline_aligned(data, strategy):\n","  counts = get_mle_counts_aligned(data['Word'], data['Label'])\n","  return strategy(counts)\n","\n","def get_rl(token, model, oov_level = 0):\n","    try:\n","        return model[token]\n","    except:\n","        return oov_level\n","\n","\n","def mle_levels_inference_pipeline(fragment, model, backoff_freq = False):\n","  tokens = [t.split('#')[0] for t in fragment.split(' ')]\n","  levels = [get_rl(token, model, 0) for token in tokens]\n","  levels = [round(a) if a > 3 else 3 for a in levels]\n","  return levels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPCGdSvP9PX6"},"outputs":[],"source":["highest_aligned_model = mle_training_pipeline_aligned(train_aligned, max_frequency_strategy)\n","weighted_aligned_model = mle_training_pipeline_aligned(train_aligned, weighted_average_strategy)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711554013852,"user":{"displayName":"Juan Piñeros Liberato","userId":"07905555678439108668"},"user_tz":-240},"id":"mkLq1Ewh-Pim"},"outputs":[],"source":["def get_gt_levels(fragment):\n","  return [int(t.split('#')[1]) for t in fragment.split(' ')]\n","\n","gt_levels = np.concatenate([get_gt_levels(f) for f in frag_test['0']])"]},{"cell_type":"markdown","metadata":{"id":"JoMp_Lt2_JhG"},"source":["#### Only eight experiments. Given that we are checking on word level, aggregation is not needed as an experimental variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6TxvmS4-rrW"},"outputs":[],"source":["res = [ np.concatenate([mle_levels_inference_pipeline(a, highest_aligned_model) for a in frag_test['0']])\n","]\n","\n","def results_to_csv(result_arr):\n","  all_rows = []\n","  for resu in result_arr:\n","    inv_report = classification_report(gt_levels, resu, output_dict = True)\n","\n","    arr_inv = np.concatenate([[inv_report[x]['f1-score'],\n","            inv_report[x]['precision'],\n","            inv_report[x]['recall'],] for x in ['3', '4', '5']])\n","    arr_inv = np.append(arr_inv, inv_report['accuracy'])\n","    arr_inv = np.append(arr_inv, inv_report['macro avg']['f1-score'])\n","\n","\n","\n","    all_rows.append(arr_inv)\n","\n","  return all_rows\n","\n","pd.DataFrame(results_to_csv(res), \n","             columns= ['f1_3','3_prec','3_recall','f1_4','4_prec','4_recall','f1_5','5_prec','5_recall','accuracy','f1_macro', 'binsize'])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMXj7rH7lfCaHIms3zGDedU","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
